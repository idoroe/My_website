---
title: "LabelForge"
description: "A full-stack multi-user annotation platform with 3 role-based workflows, a task state machine enforcing valid transitions at the API level, customizable label buttons with keyboard shortcuts, and a quality dashboard with per-annotator metrics — built from real experience as an AI model trainer at Mindrift."
heroImage: "/screenshots/labelforge-hero.webp"
tags: ["React", "TypeScript", "Django REST Framework", "PostgreSQL", "JWT Auth", "Docker"]
category: "Full-Stack"
featured: true
sortOrder: 2
problem: "I spent months as an AI model trainer at Mindrift AI, evaluating LLM outputs and reviewing annotations. The workflow was always the same: claim a task, annotate it, submit for review, get feedback, iterate. But the tools we used were clunky — no quality metrics, no clear rejection feedback, no way to see how the team was performing."
role: "Full-Stack Developer"
techStack: ["React", "TypeScript", "Django REST Framework", "PostgreSQL", "JWT Auth", "Docker"]
architecture: "React + TypeScript frontend communicates with a Django REST Framework backend backed by PostgreSQL and secured with JWT authentication. Three role-based workflows (Annotator, Reviewer, Admin) are enforced at the API level, with a task state machine governing valid transitions (unclaimed → in-progress → submitted → approved/rejected)."
challenges: "Designing a task state machine that enforces valid transitions at the API level while supporting three distinct role-based workflows, and building a quality dashboard with per-annotator metrics that gives project managers real visibility into annotation accuracy and throughput."
shipped: "Full-stack annotation platform with 3 role-based workflows (Annotator, Reviewer, Admin) and a task state machine enforcing valid transitions at the API level. Customizable label buttons and keyboard shortcuts enhancing usability for 15+ reviewers, leading to a 30% decrease in revision requests. Quality dashboard with per-annotator metrics (rejection rate, avg time, throughput), daily approval charts, and label-distribution breakdowns filterable by project."
impact: "Customizable label buttons and keyboard shortcuts enhanced usability for 15+ reviewers, leading to a 30% decrease in revision requests. The quality dashboard gives project managers instant visibility into per-annotator performance, rejection rates, and throughput."
liveUrl: ""
githubUrl: "https://github.com/idoroe/LabelForge"
screenshots: ["/screenshots/labelforge-annotate.webp", "/screenshots/labelforge-quality.webp", "/screenshots/labelforge-review.webp", "/screenshots/labelforge-admin.webp"]
---

## The Story

I spent months as an AI model trainer at Mindrift AI, evaluating LLM outputs and reviewing annotations across datasets. The workflow was always the same: claim a task, annotate it, submit for review, get feedback, iterate. But the tools we used were clunky — no quality metrics, no clear rejection feedback, no way to see how the team was performing.

LabelForge is the platform I wished existed. It's a full-stack annotation platform built with React, TypeScript, Django REST Framework, and PostgreSQL that supports three role-based workflows (Annotator, Reviewer, Admin) with JWT-secured API access. The dashboard tracks rejection rates, throughput, and per-annotator performance in real time.

This isn't a theoretical exercise — it's built from real experience with real annotation workflows.

## Task State Machine

The core of LabelForge is a state machine that governs every task's lifecycle at the API level:

```
unclaimed → in-progress → submitted → approved
                                    ↘ rejected → in-progress (back to annotator)
```

You can't approve an unclaimed task. You can't submit a task you haven't claimed. You can't reject a task that's already approved. Every invalid transition returns a clear error — no silent data corruption. The Django REST Framework serializers and viewsets enforce these rules consistently across all endpoints.

## Three Roles, Three Different Experiences

**Annotators** see their personal task queue. They can claim unclaimed tasks, annotate them using customizable label buttons (with keyboard shortcuts for speed), and submit. If a task was previously rejected, the reviewer's feedback appears prominently so the annotator knows exactly what to fix.

**Reviewers** see a queue of submitted tasks. For each one, they see the original text and the annotator's chosen label. One click to approve. Reject requires a comment explaining why. Keyboard shortcuts let reviewers approve, reject, and navigate without touching the mouse — enhancing usability for 15+ reviewers and leading to a 30% decrease in revision requests.

**Admins** create projects, upload task datasets in bulk, and see the quality dashboard. They manage the system, not individual tasks.

JWT authentication secures every API endpoint, and Django's permission system enforces role boundaries. An annotator can't hit the review endpoint even if they construct the request manually.

## Annotation Interface

The annotation UI displays the text to be labeled, customizable label buttons defined per project, and keyboard shortcuts for fast annotation. A built-in timer tracks time spent per task. If a task was previously rejected, the reviewer's feedback is displayed prominently so the annotator knows exactly what to fix.

## Quality Dashboard

The dashboard is filterable by project and tracks:

- **Per-annotator metrics** — rejection rate, average time per task, throughput
- **Daily approval charts** — visualize team velocity over time
- **Label-distribution breakdowns** — donut chart showing annotation distribution
- **Completion rate** and overall rejection rate

This gives project managers instant visibility into data quality and annotator performance — something most annotation tools handle poorly.

## What I'd Add Next

- Named Entity Recognition (NER) annotation with text highlighting
- Inter-annotator agreement scoring (Cohen's Kappa)
- CSV/JSON export of completed annotations with quality metadata
- Real-time WebSocket notifications for task status changes
