---
title: "LabelForge"
description: "A full-stack collaborative AI training data labeling tool with real-time multi-user annotation, WebSocket-powered conflict-free editing, and reviewer-productivity features — built from real experience as an AI model trainer at Mindrift."
heroImage: "/screenshots/labelforge-hero.webp"
tags: ["React", "Node.js", "PostgreSQL", "WebSockets", "Docker"]
category: "Full-Stack"
featured: true
sortOrder: 2
problem: "I spent months as an AI model trainer at Mindrift AI, evaluating LLM outputs and reviewing annotations. The workflow was always the same: claim a task, annotate it, submit for review, get feedback, iterate. But the tools we used were clunky — no quality metrics, no clear rejection feedback, no way to see how the team was performing."
role: "Full-Stack Developer"
techStack: ["React", "Node.js", "PostgreSQL", "WebSockets", "Docker"]
architecture: "React frontend communicates with a Node.js backend backed by PostgreSQL. WebSockets enable real-time collaborative editing so up to 5 concurrent annotators can work on the same task without overwriting each other's work. Conflict-free update logic ensures data integrity across sessions."
challenges: "Implementing conflict-free real-time collaboration across multiple concurrent annotators using WebSockets, and building reviewer-productivity features (keyboard shortcuts, disagreement heatmaps, export scripts) that meaningfully cut manual workflow steps."
shipped: "Full-stack collaborative labeling tool supporting parallel annotation of 100+ items per session for text and coding tasks. Real-time multi-user editing via WebSockets. Reviewer-productivity features including keyboard shortcuts, disagreement heatmaps, and export scripts that cut batch preparation from ~10 manual actions to 3 automated ones."
impact: "Teams can annotate and review training data collaboratively in real time. Reviewer-productivity features cut the steps to prepare a training batch from about 10 manual actions to 3 automated ones."
liveUrl: ""
githubUrl: "https://github.com/idoroe/LabelForge"
screenshots: ["/screenshots/labelforge-annotate.webp", "/screenshots/labelforge-quality.webp", "/screenshots/labelforge-review.webp", "/screenshots/labelforge-admin.webp"]
---

## The Story

I spent months as an AI model trainer at Mindrift AI, evaluating LLM outputs and reviewing annotations across datasets. The workflow was always the same: claim a task, annotate it, submit for review, get feedback, iterate. But the tools we used were clunky — no quality metrics, no clear rejection feedback, no way to see how the team was performing.

LabelForge is the platform I wished existed. It's a full-stack collaborative labeling tool built with React, Node.js, and PostgreSQL that supports parallel annotation of 100+ items per session for text and coding tasks. The dashboard tracks rejection rates, throughput, and per-annotator performance in real time.

This isn't a theoretical exercise — it's built from real experience with real annotation workflows.

## Real-Time Collaborative Annotation

The core differentiator is real-time collaboration. I integrated WebSockets and conflict-free update logic so up to 5 concurrent annotators can edit the same task in real time without overwriting each other's work. No more "someone else is working on this" lockouts — everyone can contribute simultaneously, and the system handles merge conflicts automatically.

## Annotation Interface

The annotation UI displays the text to be labeled, a set of label buttons defined per dataset (e.g., Positive/Negative/Neutral), and keyboard shortcuts for fast annotation. A built-in timer tracks time spent per task. If a task was previously rejected, the reviewer's feedback is displayed prominently so the annotator knows exactly what to fix.

## Reviewer Productivity Features

This is where my experience at Mindrift directly shaped the product. I built features that address the exact pain points I had as a reviewer:

- **Keyboard shortcuts** — approve, reject, and navigate without touching the mouse
- **Disagreement heatmaps** — visually highlight items where annotators disagree, so reviewers focus attention where it matters
- **Export scripts** — automated batch preparation that cuts the steps from about 10 manual actions to 3 automated ones

These aren't nice-to-haves. They're the difference between a tool people tolerate and one they actually want to use.

## Quality Dashboard

The dashboard tracks:

- **Completion rate** and daily throughput (bar chart)
- **Rejection rate** across the project
- **Per-annotator stats:** tasks completed, rejection rate, average time
- **Label distribution** (donut chart)

This gives project managers immediate visibility into data quality and annotator performance — something most annotation tools handle poorly.

## What I'd Add Next

- Named Entity Recognition (NER) annotation with text highlighting
- Inter-annotator agreement scoring (Cohen's Kappa)
- Role-based access control for different project configurations
- CSV/JSON export of completed annotations with quality metadata
