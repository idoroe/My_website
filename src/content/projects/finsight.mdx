---
title: "FinSight"
description: "An end-to-end data pipeline that transforms raw banking transactions through a dimensional model, detects anomalies using machine learning, and explains each flag with human-readable reasons."
heroImage: "/screenshots/finsight-hero.webp"
tags: ["Python", "FastAPI", "DuckDB", "dbt", "scikit-learn", "React", "Docker"]
category: "ML"
featured: true
sortOrder: 1
problem: "Before starting my data engineering internship at RBC Borealis, I wanted to deeply understand what a production-grade financial data pipeline looks like — not just the ML model, but the entire journey from raw data to actionable insights."
role: "Data Engineer / ML Engineer"
techStack: ["Python", "FastAPI", "DuckDB", "dbt", "scikit-learn", "React", "Docker", "Isolation Forest"]
architecture: "An end-to-end anomaly-detection pipeline that ingests 50K synthetic banking transactions, transforms them through a dbt dimensional model (staging → intermediate → marts), and scores them with an Isolation Forest via FastAPI. A DuckDB star schema (fact + 3 dimension tables) with dbt-managed transformations and data quality tests powers a React dashboard with anomaly explanations ranked by severity."
challenges: "Designing a meaningful star schema that mirrors real data warehouse patterns, and building an explainability layer that generates specific, contextual reasons for each flagged transaction instead of just outputting raw anomaly scores."
shipped: "End-to-end anomaly-detection pipeline ingesting 50K synthetic banking transactions, a DuckDB star schema (fact + 3 dimension tables) with dbt-managed transformations and data quality tests, and a containerized Docker Compose stack with single-command startup that runs data generation, dbt transforms, model training, and scoring automatically on first boot."
impact: "Each flagged transaction shows human-readable explanations like 'this transaction is 7x this customer's average, occurred at 2 AM, and is their first international purchase' — turning opaque ML scores into actionable analyst insights."
liveUrl: ""
githubUrl: "https://github.com/idoroe/Finsight-Lite"
screenshots: ["/screenshots/finsight-dashboard.webp", "/screenshots/finsight-anomalies.webp", "/screenshots/finsight-transactions.webp"]
---

## The Story

Before starting my data engineering internship at RBC Borealis, I wanted to deeply understand what a production-grade financial data pipeline looks like — not just the ML model, but the entire journey from raw data to actionable insights.

FinSight ingests synthetic banking transactions, transforms them through a dimensional data model using dbt, trains an Isolation Forest to detect anomalies, and surfaces flagged transactions through an interactive dashboard. Each anomaly comes with human-readable explanations — not just "this is suspicious," but "this transaction is 7x this customer's average, occurred at 2 AM, and is their first international purchase."

The goal wasn't just to build a model. It was to build the system around the model — proper data transformations, quality tests, a serving API, and a frontend that makes the output useful to a human analyst.

## Dimensional Data Model

The pipeline uses a star schema with `fact_transactions` at the center and three dimension tables: `dim_customer`, `dim_merchant`, and `dim_time`. This isn't just a flat CSV dump — it's modeled the way real data warehouses work, with derived fields like customer risk tiers, merchant categories, and time features (`is_weekend`, `is_business_hours`).

dbt handles the transformation pipeline across three layers: staging (clean and cast), intermediate (enrich and join), and marts (final tables ready for analytics and ML).

## Anomaly Detection

An Isolation Forest model trained on transaction features including amount z-scores, transaction velocity, time-of-day patterns, and merchant novelty. The model scores every transaction with a continuous anomaly score, and a threshold flags the top ~3% as suspicious.

## Explainability — The Highlight

Each flagged transaction gets a "top 3 reasons" breakdown:

- **Unusual Amount** — "$4,892 is 7.2x above this customer's average of $679"
- **Unusual Time** — "Transaction at 2:47 AM; customer typically transacts 9AM–6PM"
- **International Activity** — "First international transaction; no prior history"

These aren't generic labels. They're generated by comparing each transaction's features against the customer's historical baseline, producing specific, contextual explanations a human analyst can act on.

## Dashboard

The React frontend surfaces a transaction feed with anomaly highlighting, summary statistics, and a detail view for each flagged transaction. The anomaly detail page shows the score, top reasons (as horizontal severity bars), and a sparkline of the customer's recent history for context.

## What I'd Add Next

- Real-time streaming ingestion with Kafka
- Model drift detection comparing score distributions over time
- SHAP values for deeper feature attribution
- Multi-model ensemble (Isolation Forest + Autoencoder)
