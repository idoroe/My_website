---
title: "FinSight"
description: "An end-to-end data pipeline that transforms raw banking transactions through a dimensional model, detects anomalies using machine learning, and explains each flag with human-readable reasons."
heroImage: "/post_img.webp"
tags: ["Python", "dbt", "DuckDB", "FastAPI", "React", "scikit-learn"]
category: "ML"
featured: true
sortOrder: 1
problem: "Before starting my data engineering internship at RBC Borealis, I wanted to deeply understand what a production-grade financial data pipeline looks like — not just the ML model, but the entire journey from raw data to actionable insights."
role: "Data Engineer / ML Engineer"
techStack: ["Python", "dbt", "DuckDB", "FastAPI", "React", "scikit-learn", "Isolation Forest"]
architecture: "Raw CSV transactions flow through a 3-layer dbt pipeline (staging → intermediate → marts) into DuckDB, then an Isolation Forest model scores anomalies. A FastAPI backend serves scored transactions to a React dashboard with explainable reason breakdowns."
challenges: "Designing a meaningful star schema that mirrors real data warehouse patterns, and building an explainability layer that generates specific, contextual reasons for each flagged transaction instead of just outputting raw anomaly scores."
shipped: "Full end-to-end pipeline: dimensional data model with fact_transactions and 3 dimension tables, Isolation Forest trained on 50K synthetic transactions flagging the top ~3% as suspicious, and a React dashboard with anomaly detail views showing top 3 deviation reasons."
impact: "Each flagged transaction shows human-readable explanations like 'this transaction is 7x this customer's average, occurred at 2 AM, and is their first international purchase' — turning opaque ML scores into actionable analyst insights."
liveUrl: ""
githubUrl: "https://github.com/idoroe/finsight"
screenshots: []
---

## The Story

Before starting my data engineering internship at RBC Borealis, I wanted to deeply understand what a production-grade financial data pipeline looks like — not just the ML model, but the entire journey from raw data to actionable insights.

FinSight ingests synthetic banking transactions, transforms them through a dimensional data model using dbt, trains an Isolation Forest to detect anomalies, and surfaces flagged transactions through an interactive dashboard. Each anomaly comes with human-readable explanations — not just "this is suspicious," but "this transaction is 7x this customer's average, occurred at 2 AM, and is their first international purchase."

The goal wasn't just to build a model. It was to build the system around the model — proper data transformations, quality tests, a serving API, and a frontend that makes the output useful to a human analyst.

## Dimensional Data Model

The pipeline uses a star schema with `fact_transactions` at the center and three dimension tables: `dim_customer`, `dim_merchant`, and `dim_time`. This isn't just a flat CSV dump — it's modeled the way real data warehouses work, with derived fields like customer risk tiers, merchant categories, and time features (`is_weekend`, `is_business_hours`).

dbt handles the transformation pipeline across three layers: staging (clean and cast), intermediate (enrich and join), and marts (final tables ready for analytics and ML).

## Anomaly Detection

An Isolation Forest model trained on transaction features including amount z-scores, transaction velocity, time-of-day patterns, and merchant novelty. The model scores every transaction with a continuous anomaly score, and a threshold flags the top ~3% as suspicious.

## Explainability — The Highlight

Each flagged transaction gets a "top 3 reasons" breakdown:

- **Unusual Amount** — "$4,892 is 7.2x above this customer's average of $679"
- **Unusual Time** — "Transaction at 2:47 AM; customer typically transacts 9AM–6PM"
- **International Activity** — "First international transaction; no prior history"

These aren't generic labels. They're generated by comparing each transaction's features against the customer's historical baseline, producing specific, contextual explanations a human analyst can act on.

## Dashboard

The React frontend surfaces a transaction feed with anomaly highlighting, summary statistics, and a detail view for each flagged transaction. The anomaly detail page shows the score, top reasons (as horizontal severity bars), and a sparkline of the customer's recent history for context.

## What I'd Add Next

- Real-time streaming ingestion with Kafka
- Model drift detection comparing score distributions over time
- SHAP values for deeper feature attribution
- Multi-model ensemble (Isolation Forest + Autoencoder)
